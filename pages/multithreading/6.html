<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link rel="stylesheet" href="/css/page.css">
    <title>Document</title>
</head>

<body>
    <section>
        <div class="first">
            <h2>PROGRAM</h2>
            <h4> 
              //implement a concurrent web crawler that crawls multiple websites simultaneously using threads. 
           </h4>
<pre>
  // File: ConcurrentWebCrawler.java

import java.io.BufferedReader;
import java.io.InputStreamReader;
import java.net.HttpURLConnection;
import java.net.URL;

// Each thread crawls one URL
class WebCrawlerThread extends Thread {
    private String url;

    public WebCrawlerThread(String url) {
        this.url = url;
    }

    public void run() {
        try {
            System.out.println("ğŸ”— Crawling: " + url);

            // Open connection
            URL site = new URL(url);
            HttpURLConnection conn = (HttpURLConnection) site.openConnection();
            conn.setRequestMethod("GET");
            conn.setConnectTimeout(5000); // 5 seconds timeout
            conn.setReadTimeout(5000);

            // Read response
            BufferedReader reader = new BufferedReader(new InputStreamReader(conn.getInputStream()));
            String line;
            StringBuilder content = new StringBuilder();

            int lines = 0;
            while ((line = reader.readLine()) != null && lines &lt; 5) { // Read only first 5 lines
                content.append(line).append("\n");
                lines++;
            }
            reader.close();

            // Print first few lines of content
            System.out.println("âœ… " + url + " fetched successfully. Showing first few lines:\n");
            System.out.println(content.toString());
            System.out.println("------------------------------------------------------------\n");

        } catch (Exception e) {
            System.out.println("âŒ Failed to crawl: " + url + " | " + e.getMessage());
        }
    }
}

public class ConcurrentWebCrawler {
    public static void main(String[] args) throws InterruptedException {
        // List of websites to crawl
        String[] urls = {
            "https://www.example.com",
            "https://www.wikipedia.org",
            "https://www.openai.com",
            "https://www.oracle.com",
            "https://www.bbc.com"
        };

        // Create and start threads
        WebCrawlerThread[] threads = new WebCrawlerThread[urls.length];
        for (int i = 0; i &lt; urls.length; i++) {
            threads[i] = new WebCrawlerThread(urls[i]);
            threads[i].start();
        }

        // Wait for all threads to finish
        for (int i = 0; i &lt; urls.length; i++) {
            threads[i].join();
        }

        System.out.println("ğŸŒ Crawling complete! All sites processed.");
    }
}    
     
</pre>
        </div>
        <div>
           <h2>OUTPUT</h2>
<pre>
   ğŸ”— Crawling: https://www.example.com
ğŸ”— Crawling: https://www.wikipedia.org
ğŸ”— Crawling: https://www.openai.com
ğŸ”— Crawling: https://www.oracle.com
ğŸ”— Crawling: https://www.bbc.com
âœ… https://www.example.com fetched successfully. Showing first few lines:

<!doctype html>
<html>
<head>
    <title>Example Domain</title>
    <meta charset="utf-8" />
------------------------------------------------------------

âœ… https://www.wikipedia.org fetched successfully. Showing first few lines:

<!DOCTYPE html>
<html class="client-nojs" lang="en" dir="ltr">
<head>
    <meta charset="UTF-8"/>
------------------------------------------------------------

ğŸŒ Crawling complete! All sites processed.
                  
</pre>

        </div>
    </section>

</body>

</html>